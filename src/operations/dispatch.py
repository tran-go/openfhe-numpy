# === dispatch.py ===
from typing import Tuple, Callable, Any
import functools
import os
import inspect


# Operation registry - stores implementations keyed by (operation_name, signature)
TENSOR_FUNCTIONS = {}

# Registry for explicitly commutative operations
COMMUTATIVE_OPS = set()


def mark_commutative(*names):
    """Mark one or more operations as commutative."""
    COMMUTATIVE_OPS.update(names)


def register_tensor_function(name, type_signatures):
    """Register operation implementation for specific types.

    Supports both single and multiple type signatures.
    Also attaches function metadata like docstring and return type.
    """
    import inspect
    from typing import get_type_hints

    def decorator(func):
        func._tensor_function_name = name
        func._tensor_function_docstring = inspect.getdoc(func)
        func._tensor_function_type_hints = get_type_hints(func)
        if isinstance(type_signatures, tuple):
            signatures = [type_signatures]
        else:
            signatures = type_signatures

            for signature in signatures:
                TENSOR_FUNCTIONS[(name, signature)] = func
            return func

    return decorator


def dispatch_tensor_function(
    func_name: str, args: Tuple[Any, ...], kwargs=None, return_hint=False
) -> Any:
    """Dispatch implementation for the given function name and argument types.

    If return_hint is True, return a string describing what failed instead of raising.
    """
    kwargs = kwargs or {}

    # Step 1: Extract type names
    sig = tuple(getattr(arg, "dtype", type(arg).__name__) for arg in args)

    if os.getenv("DISPATCH_DEBUG", "0") == "1":
        print(f"[DISPATCH] Trying {func_name}{sig}")

    # Step 2: Try exact match
    key = (func_name, sig)
    if key in TENSOR_FUNCTIONS:
        return TENSOR_FUNCTIONS[key](*args, **kwargs)

    # Step 3: Normalize scalars to 'scalar'
    normalized_sig = tuple("scalar" if t in {"int", "float", "complex", "bool"} else t for t in sig)
    key = (func_name, normalized_sig)
    if key in TENSOR_FUNCTIONS:
        return TENSOR_FUNCTIONS[key](*args, **kwargs)

    # Step 4: Try commutative fallback
    if func_name in COMMUTATIVE_OPS and len(args) == 2:
        a_pri = getattr(args[0], "tensor_priority", 0)
        b_pri = getattr(args[1], "tensor_priority", 0)
        if b_pri > a_pri:
            swapped_sig = (normalized_sig[1], normalized_sig[0])
            key = (func_name, swapped_sig)
            if key in TENSOR_FUNCTIONS:
                return TENSOR_FUNCTIONS[key](args[1], args[0], **kwargs)

    # Step 5: Try MRO fallback on the first argument
    if hasattr(args[0], "__class__"):
        for parent in args[0].__class__.__mro__[1:]:
            if parent.__name__ == "object":
                continue
            parent_sig = (parent.__name__,) + normalized_sig[1:]
            key = (func_name, parent_sig)
            if key in TENSOR_FUNCTIONS:
                return TENSOR_FUNCTIONS[key](*args, **kwargs)

    if return_hint:
        return (
            f"No implementation found for {func_name} with signature {sig}.\n"
            f"Registered for {func_name}: {[k[1] for k in TENSOR_FUNCTIONS if k[0] == func_name]}"
        )

    raise NotImplementedError(
        f"{str(func_name)} not implemented for types {sig}.\n"
        f"Registered signatures for '{func_name}':\n  "
        + "\n  ".join(str(k[1]) for k in TENSOR_FUNCTIONS if k[0] == func_name)
    )


def tensor_function_api(op_name: str, binary: bool = True) -> Callable:
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            if binary:
                if len(args) < 2:
                    raise TypeError(f"{op_name} requires 2 arguments")
                a, b = args[0], args[1]

                # Check if both have __tensor_function__
                a_tf = hasattr(a, "__tensor_function__")
                b_tf = hasattr(b, "__tensor_function__")

                if a_tf and b_tf:
                    # Use tensor_priority to decide which implementation to use
                    a_pri = getattr(a, "tensor_priority", 0)
                    b_pri = getattr(b, "tensor_priority", 0)

                    if b_pri > a_pri:
                        return b.__tensor_function__(op_name, args, kwargs)
                    else:
                        return a.__tensor_function__(op_name, args, kwargs)

                # Original code continues...
                if a_tf:
                    return a.__tensor_function__(op_name, args, kwargs)
                if b_tf:
                    return b.__tensor_function__(op_name, args, kwargs)

                return dispatch_tensor_function(op_name, args, kwargs)

            else:
                if not args:
                    raise TypeError(f"{op_name} requires at least 1 argument")
                a = args[0]

                if hasattr(a, "__tensor_function__"):
                    return a.__tensor_function__(op_name, args, kwargs)

                return dispatch_tensor_function(op_name, args, kwargs)

        return wrapper

    return decorator


# === Mark commutative operations ===
mark_commutative("add")
